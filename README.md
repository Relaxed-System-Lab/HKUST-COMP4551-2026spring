# HKUST-COMP4551-2026spring

</div>

<h2 style="text-align: center;"> Large-Scale Machine Learning for Foundation Models </h2>

**Lecturer**: [Binhang Yuan](https://binhangyuan.github.io/site/). 

**Teaching Assistant**: TBD


## Overview

In recent years, foundation models have fundamentally revolutionized the state-of-the-art of artificial intelligence. Thus, the computation in the training or inference of the foundation model could be one of the most important workflows running on top of modern computer systems. This course unravels the secrets of the efficient deployment of such workflows from the system perspective. Specifically, we will i) explain how a modern machine learning system (i.e., PyTorch) works; ii) understand the performance bottleneck of machine learning computation over modern hardware (e.g., Nvidia GPUs); iii) discuss four main parallel strategies in foundation model training (data-, pipeline-, tensor model-, optimizer- parallelism, etc.); iv) real-world deployment of foundation model including efficient inference and fine-tuning.


## Syllabus 


| Date | Topic |
|-----|------|
|W1 - 02/03,02/05 | - Introduction and Logistics [[Slides](https://github.com/Relaxed-System-Lab/HKUST-COMP4551-2026spring/blob/main/Lecture%201%20-%20Introduction%20and%20Logistics.pdf)]  <br> - ML Preliminary [[Slides](https://github.com/Relaxed-System-Lab/HKUST-COMP4551-2026spring/blob/main/Lecture%201%20-%20Introduction%20and%20Logistics.pdf)] |
|W2 - 02/10,02/12 | - Stochastic Gradient Descent <br> - Automatic Differentiation |
|W3 - 02/17,02/19 | - Spring Festival |
|W4 - 02/24,02/26 | - Language Model Architecture <br> - Large Scale Pretrain Overview  |
|W5 - 03/03,03/05 | - Nvidia GPU Performance      <br> - Collective Communication Library |
|W6 - 03/10,03/12 | - Data-, Pipeline- Parallel Training  <br> - Tensor Model-, Optimizer- Parallel Training |
|W7 - 03/17,03/19 | - Sequence-, MoE- parallelism  <br> - Mid-Term Review  |
|W8 â€“ 03/24,03/26 | - Mid-Term Exam <br> - Generative Inference   |
|W9 - 03/31,04/02 | - Inference Alogirhtm Optimizations  <br> - Inference System Optimizations |
|W10 - 04/07,04/09| - Spring Break <br> - Prompt Engineering   |
|W11 - 04/14,04/16 | - Inference Scaling <br> - Retrieval Augmented Generation   |
|W12 - 04/21,04/23 | - LLM Agent <br> - Parameter Efficient Fine-Tuning |
|W13 - 04/28, 04/30| - RL Alignment <br> - LLM Evaluation |
|W14 - 05/05,05/07 | - Guest Speech  <br> - Final Review |


## Grading Policy
- 4 Homework (4 $\times$ 5% $=$ 20%);
- Mid-term exam (30%);
- Final exam (50%).

## Homework 
| Topic | Release |   Due   |
|-------|---------|---------|
| Homework1 |2025/02/13 | 2025/02/22 |
| Homework2 |2025/03/04 | 2025/03/12 |
| Homework3 |2025/03/25 | 2025/04/10 |
| Homework4 |2025/04/22 | 2025/05/06 |




